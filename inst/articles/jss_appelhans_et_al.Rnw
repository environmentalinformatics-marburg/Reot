\documentclass[article]{jss}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{esint}
\usepackage{float}
\usepackage[authoryear]{natbib}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 %\usepackage{Sweave}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\title{\pkg{Reot}: Empirical Orthogonal Teleconnections in \proglang{R}}


\author{Tim Appelhans, Florian Detsch, Thomas Nauss\\
Environmental Informatics, Philipps University Marburg}

\Plaintitle{Reot: Empirical Orthogonal Teleconnections in R}
\Shorttitle{Empirical Orthogonal Teleconnections in R}

\Plainauthor{Tim Appelhans}

\Abstract{In climate science, teleconnection analysis has a long standing history as a means for describing regions that exhibit above average capability of explaining variance over time within a certain spatial domain (e. g. global). The most prominent example of a global coupled ocean-atmosphere teleconnection is the El Nino Southern Oscillation (ENSO). There are numerous signal decomposition methods for identifying such regions, the most widely used of which are (rotated) empirical orthogonal functions (EOF). First introduced by \cite{Dool2000}, empirical orthogonal teleconnections (EOT) denote a regression based approach that allows for straight-forward interpretation of the extracted modes. In this paper we present the \proglang{R} \citep{Rcore2013} implementation of the original algorithm in the \pkg{Reot} package. To highlight its usefulness, we provide 3 examples of potential use-case scenarios for the method including the replication of one of the original examples from \cite{Dool2000}. Furthermore, we highlight the algorithm's use for cross correlations between two different geographic fields (identifying SST drivers for precipitation), as well as statistical downscaling from coarse to fine grids (using NDVI fields).}

\Keywords{teleconnection analysis, spatial data mining, raster, \proglang{R}, climate science}

\Plainkeywords{teleconnections, spatial data mining, raster, R, climate science}

\Address{Tim Appelhans\\
Department of Geography\\
Environmental Informatics\\
Philipps Universität Marburg\\
Deutschhausstraße 12\\
35032 Marburg\\
Germany\\
E-mail: \email{tim.appelhans@staff.uni-marburg.de}\\
URL: \url{http://environmentalinformatics-marburg.de}}

%% publication information
%% NOTE: Typically, this can be left commented and will be filled out by the technical editor
%% \Volume{}
%% \Issue{}
%% \Month{}
%% \Year{}
%% \Submitdate{}
%% \Acceptdate{}

\begin{document}

<<setup,cache=FALSE,include=FALSE>>=
opts_chunk$set(echo=TRUE, fig.path='figure/Rfig-', cache.path='cache-jss/', cache=FALSE, out.width='.49\\linewidth', results='asis', tidy = FALSE, fig.width=5, fig.height=5)
render_sweave() # use boring Sweave environments
set_header(highlight = '') # do not use the Sweave.sty package
pdf.options(family = 'Palatino')
options(width = 84, digits=3, prompt='R> ', continue = "+  ")
library(formatR)
library(Reot)
#set.seed(31415)  # for reproducibility
knit_hooks$set(custom.plot = hook_plot_custom)
@


\section{Introduction}
With more than 30 years of continuous satellite observations and an ever increasing amount of available environmental models (both statistical and numerical), time series analysis of gridded geoscientific data has become more and more popular in the recent past. One field where spatio-temporal analysis of gridded data has a long history is climate science. Due to the nature of the field, many studies are carried out on rather large spatial extents (e. g. global scales) and hence gridded data sets have been utilized intensively. Yet, even on smaller regional or local scales, high resolution satellite imagery, gridded interpolated point measurements or numerical model grids are able to provide very useful information that is spatially and temporally consistent. 

In climate science, teleconnection analysis has a long standing history as a means for describing regions that exhibit above average capability of explaining variance over time within a certain spatial domain (e.g. global). The most prominent example of a global atmospheric teleconnection is the El Nino Southern Oscillation (ENSO). There are numerous signal decomposition methods for identifying such regions, the most widely used of which are (rotated) empirical orthogonal functions (EOF). First introduced by \cite{Dool2000}, empirical orthogonal teleconnections (EOT) denote a correlation based approach that allows for straight-forward interpretation of the extracted modes. In comparison to EOF analysis, it has had a rather shadowy existence since its introduction to the scientific community. There are, however, a few studies that have made use of EOT analysis in several ways. \cite{Franzke2002} used EOTs to characterise low-frequency flow variablity modes and their influence on storm tracks in the Northern Hemisphere in a simplified global circulation model. \cite{Smith2003} used both rotated EOFs and EOT analysis to investigate high-frequency anomalies in global sea surface temperatures (SSTs) and found that both approaches produce many patterns that are almost identical. To characterise drought severity accross Europe between 1901 and 2002 \cite{Schrier2006} used EOT analysis for both spatial pattern isolation and time series analysis of these patterns. Yet another example of the use of EOTs can be found in \cite{Reynolds2007}. Here, EOTs are used to bias correct satellite imagery as part of a methodology to create high-resolution (in space and time) SST grids. \cite{Rotstayn2010} utilised EOT analysis in order to evaluate the performance of a newly adapted coupled ocean–atmosphere global climate model, especially its ability to reporduce leading modes of precipitation variability in Australia. Also using EOT analysis \cite{Klingaman2013} identified regions of high rainfall variability in Queensland, Australia, which they then related to large-scale drivers such as ENSO. 

Despite the apparent diversity in the way of applying EOT analysis, all of the above mentioned investigations are rooted within climate science. We believe that the potential of the EOT algorithm is much greater and that it can also be useful for other disciplines that make use of gridded data sets. Therefore, we hope that with a computational efficient implementation of the EOT algorithm in the open source package \pkg{Reot} this methodology will be adopted more widely in the future, potentially also by disciplines other than the climate science community. 

\section[Eot algorithm]{Eot algortihm\label{sec:algortihm}}
Empiricial Orthogonal Teleconnections have first been introduced to the international literature as an alternative to the classical approach of Empirical Orthogonal Functions by \cite{Dool2000}. \cite{Dool2007} outlines that both EOT and EOF are indeed very similar techniques with the former producing less abstract results. Both EOF and EOT decompose spatio-temporal fields into a set of independent orthogonal patterns. In contrast to EOFs, which are orthogonal in both space and time, EOT analysis produces patterns that are orthogonal in either space or time (the current implementation of \pkg{Reot} provides the latter). EOTs carry a quantitative meaning in the form of explained variance, thus enabling intuitive interpretation of the results.\\
It is possible to calculate internal EOTs isolating teleconnection patterns within a two dimensional spatial data array. We highlight this in section \ref{subsec:vdendoolfig6} figure \ref{fig:vdendool}. In addition, EOTs may also be used to investigate teleconnectivity between two spatio-temporal fields. In this case, temporal variability of one series (predictor) is analysed with regard to explained variance of the temporal dynamics of another series (response). Apart from similarity in the temporal dimension (i. e. identical amount of data points over time), the algorithm can be applied to any two data series without further requirements such as identical spatial resolution or physical units of the data. An example of this can be found in section \ref{sec:examples} figure \ref{fig:sstgpcp} where we explain precipitation variation as a function of sea surface temperatures (SSTs).\\
The mathematics of the EOT algorithm are described in detail in \cite{Dool2000} and \cite{Dool2007} and can be summarised as follows. First, the temporal profiles of each pixel p$_{p}$ of the predictor domain are regressed against the profiles of all pixels p$_{r}$ in the response domain (in case of only a single field p$_{r}$ = p$_{p}$ - 1). The calculated coefficients of determination are summed up and the pixel with the highest sum for explaining variance within the response domain is identified as the 'base point' of the first/leading mode. The temporal profile at this base point is the first/leading EOT. Then, the residuals from the regression are taken to be the basis for the calculation of the next EOT, thus ensuring orthogonality of the identified teleconnections. This procedure is repeated until a predefined amount of \emph{n} EOTs is calculated. In general, \pkg{Reot} implements a 'brute force' spatial data mining approach to identify locations of enhanced potential to explain spatio-temporal variability within the same or another geographic field. Both stable-release and development versions are currently hosted at \url{https://github.com/environmentalinformatics-marburg/Reot} and can be installed with the \emph{install\_github()} function available in the \pkg{devtools} package \citep{Wickham2013} by setting the respective git reference.

\section[Package design and functionality]{Package design and functionality\label{sec:functionality}}

\subsection{General package design}
Given that EOT analysis is designed for gridded space-time data, \pkg{Reot} is entirely based on the \pkg{raster} package \citep{Hijmans2013}. This means, that in order to use the functions provided by \pkg{Reot}, input must be of class \emph{Raster*}. Equivalently, all results will be of class \emph{Raster*}, including any results optionally written to disk. This ensures consistency and compatibility with all other \proglang{R} packages designed for the analysis of \emph{Raster*} data. Essentially, all functionality provided by \pkg{Reot} could be achieved with the standard functions provided by \pkg{raster}, and indeed many functions of the package at hand utilise these. However, as the nature of space-time analysis is such that data sets are commonly very large and that in order to find one base point, p$_{p}$ * p$_{r}$ computations are necessary, the regression calculations of \pkg{Reot} are implented in \proglang{C++} via \pkg{Rcpp} \citep{Eddelbuettel2011}. This ensures acceptable computation times and memory usage, even though calculations can still take time to complete, depending on the number of predictor and response pixels. In comparison to the most commonly available implementation of the EOT-algorithm in IDRISI's Earth Trends Modeller \citep[Version Taiga;][]{Eastman2009}, computation time is reduced by the order of a magnitude (though comparisons are somewhat biased by computational resources etc.). In addition to the above mentioned \pkg{raster} and \pkg{Rcpp} dependencies, \pkg{Reot} provides some plotting routines that are based on the \pkg{lattice} and \pkg{latticeExtra} packages \citep{Sarkar2008, Sarkar2013} and use palettes from \textbf{ColorBrewer.org} \citep{Harrower2003} through the \pkg{RColorBrewer} package \citep{Neuwirth2011}.

\subsection{Functionality}
Given below is a detailed list of the available \pkg{Reot} functions (in alphabetical order). Most of these will be utilised in section \ref{subsec:sstprecip}, however, the reader is referred to the documentation where in depth descriptions and detailed examples are readily available. To highlight the functionality of the \pkg{Reot} package, we provide three use-case scenarios of potential applications of the package in Section \ref{sec:examples}.

Currently, \pkg{Reot} provides the following functionality (only user relevant functions are shown):

\begin{itemize}
  \item \emph{anomalize()} - create an anomaly space-time field from a \emph{RasterStack}. Either based on the overall mean of the stack, or a supplied reference raster.
  \item \emph{denoise()} - noise filtering through principal components. The user can either specify how many components to keep or can provide a value for the minimum variance that should be kept. Additionally, the user can choose whether the field should be geographically weighted (see \emph{geoWeight()}).
  \item \emph{deseason()} - create seasonal anomalies of a RasterStack by supplying a suitable seasonal window.
  \item \emph{eot()} - the core function of the package. The user supplies a predictor stack and (optionally) a response stack, the number of \emph{n} EOT modes to be calculated, whether the results should be standardised (i.e. R$^{2}$ values be multiplied with the variance), whether both predictor and response series (or only the latter) should be reduced after the first mode is identified (i.e. the residuals be taken) and the type of the link function (either correlation or index of directional agreement). Optionally, all results can be written to disk to a user supplied path and the amount of information printed to the console can be controlled.
  \item \emph{geoWeight()} - create geographically weighted fields using the \emph{cosine} of latitude in order to compensate for non-equal area grids in case of non-projected geographical data (i. e. lat/lon coordinate reference system).
  \item \emph{lagalize()} - create time-lagged \emph{RasterStacks} by choosing a suitable lag number with regard to the frequency of the data.
  \item \emph{nEot4Var()} - identify the number of modes needed to explain a certain, user-supplied amount of variance within the response series. Note that this is a post-hoc function that needs the results returned by \emph{eot()}.
  \item \emph{plotEot()} - standard plotting routine for the results of \emph{eot()}. By default three panels are shown: i) the coefficient of determination image of the predictor series ii) the correlation coefficent image for the response series and iii) the times series of the predicor series at the base point of the identified eot mode. It is possible to control which of the \emph{n} EOT modes should be displayed and the combination of images shown is completely flexible. Additionally, the colour palette can be controlled and further minor modifications can be made (see documentation).
  \item \emph{plotLocations()} - a simple plotting routine to visualise the location of all identified base points colour coded according to EOT mode (1 to \emph{n}).
\end{itemize}

As pointed out before, both predictor as well as response data need to be of format \emph{RasterBrick} or \emph{RasterStack}. The function \emph{eot()} returns a list of \emph{n} EOTs each comprising a list of the following data:

\begin{itemize}
  \item \emph{eot.series} - the EOT time series at the identified base point.
  \item \emph{max.xy} - the cell number of the indeified base point.
  \item \emph{exp.var} - the (cumulative) explained variance of the considered EOT.
  \item \emph{loc.eot} - the location of the base point (in original coordinates).
  \item \emph{r.predictor} - the \emph{RasterLayer} of the correlation coefficients between the base point and each pixel of the predictor domain.
  \item \emph{rsq.predictor} - as above but for the coefficient of etermination.
  \item \emph{rsq.sums.predictor} - as above but for the sums of R$^{2}$.
  \item \emph{int.predictor} - the \emph{RasterLayer} of the intercept of the regression equation for each pixel of the predictor domain.
  \item \emph{slp.predictor} - same as above but for the slope of the regression equation for each pixel of the predictor domain.
  \item \emph{p.predictor} - the \emph{RasterLayer} of the significance (p-value) of the the regression equation for each pixel of the predictor domain.
  \item \emph{resid.predictor} - the \emph{RasterBrick} of the reduced data for the predictor domain.  
\end{itemize}

All \emph{*.predictor} fields are also returned for the \emph{*.response} domain, even if predictor and response domain are equal. This is due to that fact, that if not both fields are reduced after the first EOT is found, these \emph{RasterLayers} will differ. If the user chooses to write the results to disk, all \emph{Raster*} fields outlined above will be saved as native \pkg{raster} \emph{.grd} files. Location (\emph{x, y}), EOT number \emph{n}, cumulative explained variance along with a comment as to whether the location of the EOT is unambiguous will be saved in a \emph{.csv} file in a user-supplied location on the hard disk. This enables further investigations of the results without having to re-run the analysis from scratch, which, in light of the sometimes high computation times, is indispensable. Note, however, that this may require rather large amounts of disk space, as for each identified EOT 13 \emph{Raster*} objects will be written to disk.

\section[Examples]{Examples\label{sec:examples}}
This section highlights three different use-case scenarios for potential applications of the \pkg{Reot} package (though we are confident that there are many more exciting applications to be found for the algorithm).

\begin{enumerate}
  \item As a first example, we replicate one of the examples from \cite{Dool2000} (Example 3.d. - Figure 6). A spatio-temporal field of 700 mb geopotential heights of NCEP/NCAR Reanalysis grids \citep{Kalnay1996} is decomposed into its four leading modes exhibiting the prominent patterns of North Atlantic Oscillation (NAO) and Pacific-North American Pattern (PNA) as modes 1 and 2, respectively. 
  \item A second example higlights the application of a EOT analysis between two geoscientific fields. We identify influential areas of sea-surface temeperature anomalies in the tropical Pacific Ocean that remotely drive precipitation dynamics over mainland Australia. For this we use NOAA OI SST V2 \citep{Reynolds2007} as a predictor field and Global Precipitation Climatology Project data \citep[GPCP V2.2;][]{Adler2003} as the response series. The aim is to directly identify regions of sea surface temperature variability that exhibit enhanced ability of explaining precipitation variations over mainland Australia. So far, only indirect approaches have been taken \citep[e. g.][]{Klingaman2013} where internal base points of precipiation variability have been identified and then regressed against standard teleconnection indices, such as the Southern Oscillation Index (SOI).
  \item As a final example we show a completely different application of the \pkg{Reot} functionality. The intention is to spatially downsample the Global Inventory Modeling and Mapping Studies (GIMMS, V2.0) NDVI product \citep{Tucker2005} with a resolution of 8 km to MODIS NDVI MYD13Q1 observations \citep{Lpdaac2006} with a resolution of 250 m for the region of Mt. Kilimanjaro. We evaluate the quality of the resulting artificial series by comparing the calculated NDVI images with MODIS NDVI images that were not considered during model definition. Furthermore, we visually assess the performance of the calculated time series for selected pixels which are representative of a wide range of land-use types such as savanna, coffee plantation, tropical mountain forest, tropical sub-alpine and alpine vegetation zones, among others.
\end{enumerate}


\subsection[Winter mean 700 mb height over the Northern Hemisphere]{Winter mean 700 mb height over the Northern Hemisphere\label{subsec:vdendoolfig6}}
Section 3.d. in \cite{Dool2000} provides an excellent example of the use of the EOT algorithm to extract atmospheric teleconnections using winter mean 700 mb heights over the Northern Hemisphere. The climatologically inclined reader is referred to the respective section in \cite{Dool2000} for a more detailed description of the atmospheric dynamics and processes associated with the identified patterns. Here, we merely want to highlight, that the \pkg{Reot} implementation of the algorithm produces similar results to those found in \cite{Dool2000}.\\

The four modes needed for the example can be calculated with:

<<echo = TRUE, message = FALSE, fig6vdendool, fig.height=4, fig.width=6, warning=FALSE, out.width='\\linewidth', fig.align='center', fig.show = 'asis'>>=
data(vdendool)

modes <- eot(pred = vdendool, resp = NULL, n = 4, reduce.both = FALSE,
             standardised = FALSE, print.console = FALSE)
@

In order to recreate Figure 6 from \cite{Dool2000} some additional lines of code are necessary (incl. spatial reprojection of the rasters). These can be found in the supplement materials and will produce the following figure: 

\begin{figure}[H]
<<echo = FALSE, message = FALSE, prepfig6vdendool, fig.height=5, fig.width=6, warning=FALSE, out.width='\\linewidth', fig.align='center', fig.show = 'asis'>>=
library(Reot)
library(rworldmap)
library(rgdal)
library(rgeos)

data(vdendool)
data(coastsCoarse)

modes <- eot(pred = vdendool, resp = NULL, n = 4, reduce.both = FALSE,
             standardised = FALSE, print.console = FALSE)

ster <- CRS("+proj=stere +lat_0=90 +lon_0=-45")

xmin <- -180
xmax <- 180
ymin <- 20
ymax <- 90     # Coordinates for bounding box
bb <- cbind(x = c(xmin, xmin, xmax, xmax, xmin), 
            y = c(ymin, ymax, ymax, ymin, ymin))    #Create bounding box
SP <- SpatialPolygons(list(Polygons(list(Polygon(bb)), "1")), 
                      proj4string = CRS(proj4string(coastsCoarse)))

gI <- gIntersects(coastsCoarse, SP, byid = TRUE) 
out <- vector(mode = "list", length = length(which(gI))) 
ii <- 1

for (i in seq(along = gI)) if (gI[i]) {
  out[[ii]] <- gIntersection(coastsCoarse[i, ], SP)
  row.names(out[[ii]]) <- row.names(coastsCoarse)[i]
  ii <- ii + 1
}

nhem.coasts <- do.call("rbind", out)
nhem.coasts.ster <- spTransform(nhem.coasts, ster) 

lout <- list("sp.lines", nhem.coasts.ster, 
             col = "grey30", grid = TRUE)

clrs <- colorRampPalette(rev(brewer.pal(9, "RdBu")))

mode <- lapply(seq(modes), function(i) {
  projectRaster(modes[[i]]$r.predictor, crs = ster)
})

title <- lapply(seq(mode), function(i) {
  paste("Mode ", i, " : ", "EV = ", 
        round(if (i > 1) {
          modes[[i]]$exp.var * 100 - 
            modes[[i - 1]]$exp.var * 100
          } else {
            modes[[i]]$exp.var * 100
            }, 1), " : ", "BP = ", as.integer(modes[[i]]$loc.eot[, 1]), 
        ", ", as.integer(modes[[i]]$loc.eot[, 2]), sep = "")
})

p <- lapply(seq(mode), function(i) {
  spplot(mode[[i]], sp.layout = lout, main = list(title[[i]], cex = 0.7),
             col.regions = clrs(1000), at = seq(-1, 1, 0.2),
             par.settings = list(axis.line = list(col = 0)),
             colorkey = list(height = 0.75, width = 1))
})

f <- function(...) grid.arrange(..., heights = 1, ncol = 2)
do.call(f, p)   #Final plot
@


%
\caption{Replication of Figure 6 from \cite{Dool2000}. EV is explained variance of the response domain, BP is the location of the identified base point (longitude, latitude).\label{fig:vdendool}}
\end{figure}

Even though the location of the identified base points (BP) is somewhat offset, and hence the explained variance (EV) figures differ slightly, it is obvious that the isolated patterns are very similar and represent the same signals. We can only speculate as to why the base point locations differ slightly, but potential reasons may include different version numbers of the reanalysis data, rounding discrepancies between the utilised programming languages (especially when summing up the coefficients of determination) and slight differences in geographic projections.

\subsection[Identifying tropical Pacific SST drivers for Australian precipitation]{Identifying Tropical Pacific SST drivers for Australian precipitation\label{subsec:sstprecip}}
The processes of precipitation development are complex and not yet understood completely. The physical state of the atmosphere, which determines whether rain occurs or not at any point in space and time, is the result of a multitude of constatntly changing factors. Influences range from local to hemispheric boundary conditions in all 4 dimensions (incl. time). Some areas of the global oceans exhibit low-frequency anomaly signals which can influence precipitation variability world-wide. The most prominent example of a coupled ocean-atmosphere tropical SST variability is ENSO. ENSO has received much attention in the scientific literature since the major 1982–83 El Niño. Here we investigate, whether EOT analysis can be used to identify the ENSO signal as a driver for low-frequency Australian precipitation variability over the period 1982 to 2010. The data sets needed for this analysis are included in \pkg{Reot}. In order to reveal low-frequency signals such as ENSO, we need to prepare the raw data fields so that high-frequency variation is eliminated. We achieve this by creating seasonal anomalies using \emph{deseson()} and by \emph{denoise()}-ing the data to filter out some of the noise that is present in any spatio-temporal data field.
The first 3 leading modes of SSTs most influencial for Australian rainfall variability can be calculated with:

<<echo = TRUE, message = FALSE, sstgpcpprep, fig.height=6, fig.width=6, warning=FALSE, out.width='\\linewidth', fig.align='center', fig.show = 'asis', tidy = FALSE>>=
data("australiaGPCP")
data("pacificSST")

### deseason data
sst.pred <- deseason(pacificSST, cycle.window = 12)
gpcp.resp <- deseason(australiaGPCP, cycle.window = 12)

### denoise data (keeping 90 % of the variance)
sst.pred.dns <- denoise(sst.pred, expl.var = 0.9)
gpcp.resp.dns <- denoise(gpcp.resp, expl.var = 0.9)

### calculate first 3 leading modes
modes <- eot(pred = sst.pred.dns, resp = gpcp.resp.dns, n = 3, 
             standardised = FALSE, reduce.both = FALSE,
             print.console = FALSE)
@

As we can see, especially the principal components filter from \emph{denoise()} is an important step, as we need only 19 (37) of the original 348 components for the SST (GPCP) data to explain 90 \% of the respective inherent field varince.
To get a visual impression, the results for the first leading mode can be plotted using the standard \pkg{Reot} plotting routine \emph{plotEot():}
\begin{figure}[H]
<<echo = TRUE, message = FALSE, sstgpcp, fig.height=7, fig.width=6, warning=FALSE, out.width='\\linewidth', fig.align='center', fig.show = 'asis', tidy = FALSE>>=
### plot first EOT showing the location of the base point
plotEot(modes, eot = 1, show.eot.loc = TRUE, arrange = "long")
@


%
\caption{Coefficient of determination image of the first leading mode of tropical Pacific SSTs (predictor - top panel) along with correlation coefficient image of precipitaion over mainland Australia (response - central panel). The time series at the base point of this mode (grey circle in top panel) is shown in the bottom panel.\label{fig:sstgpcp}}
\end{figure}

We see that we are indeed able to isolate the ENSO signal as the most important SST driver for Australian precipitation in the tropical Pacific (EOT 1). This signal is able to explain just above 4 \% of the original variation found in rainfall over the analysed period. This may not seem much, but we need to keep in mind that precipitation is influenced by many factors, with local conditions playing a major role. Spatially, mainly the north-eastern part of the response domain is being explained with some locations showing negative correlations of up to 0.4. With regard to mainland Australia, it becomes obvious that the identified ENSO signal is not able to explain any rainfall variation in the inner-continental parts of the land mass. It is mainly the coastal areas that are influenced by the ENSO phenomenon, which is in line with the findings of \cite{Risbey2009}. Note, that our analysis did not take into account any time-lags between the SST anomalies and precipitation. Even though in this particular example lagging does not increase the explanatory power of the SST signal (not shown), it can be expected that in many cases the influence will not manifest instantaneously and that a certain lag time will explain a higher portion of the rainfall variance.

\subsection[Downscaling GIMMS NDVI to MODIS NDVI]{Downscaling GIMMS NDVI to MODIS NDVI\label{subsec:ndvidnsc}}
As much of the authors research is focused on eco-climatological studies at and around Mt. Kilimanjaro, this region shall be the focus of our last use-case. In order to infer sound trends of either climatic or closely related environmental parameters it is imperical to have a time series that spans a long enough period to capture some of the low-frequency signals that might influence their temporal dynamics. One of the parameters frequently used in eco-climatological studies is the Normalised Difference Vegetation Index (NDVI). The NDVI is a normalised difference between red and near infra-red reflectances and gives information on the 'greenness' of vegetation. Its essential characteristics are such that it ranges between -1 and 1 with dense green vegetation having positive values (close to 1) and clouds, water, ice and snow being characterised by negative values. The MODIS sensor on board NASA's Auqa satellite platform provides a standard NDVI product (MYD13Q1) that has been widely used for a diverse range of investigations. Its horizontal resolution is 250 m by 250 m and with a repeat rate of 16 days it is perfectly suited for spatio-temporal vegetation analyses that require high-resolution imagery. One of the biggest draw-backs of this data is that it is only available from mid 2002 and is therefore temporally not comprehensive enough for several research questions, such as the identification of changes over time on longer time scales or the above mentioned trend analysis. Another readily available, and also widely used NDVI data set that spans a much longer time period (1982 to 2006) is GIMMS NDVI V2.0. This data, however, has a horizontal resolution of only 8 km by 8 km. In this last eaxample we show that it is possible to utilise the \pkg{Reot} package for the spatial downscaling of GIMMS NDVI to MODIS resolution to create a consistent data set that provides high-resolution NDVI information for the period of the GIMMS data set. 

There are several commonly used approaches for statistical downscaling. Among others, these include PCA based approaches, machine learning approaches and multiple linear and non-linear regression based approaches, or combinations of these \citep[for an overview of commonly used approaches see e. g. ][]{Huth1999, Wilby1997}. Most commonly, downscaling or regionalisation of field data is achieved by using point observations as the predictand rather than response fields. Regression based approaches usually screen the predictor field(s) for suitable grid-points (pixels) to include into the multiple regression prediction based on some condition such as their significance. Machine learning approaches include Support Vector Machines \citep[SVM; e. g. ][]{Sachindra2013, Chen2010}, Artificial Neural Networaks \citep[ANN; e. g. ][]{Tripathi2006, Olsson2003} and Random Forest \citep[RF; e. g. ][]{Davy2010, Vaca2011}. The specifics of the methodologies differ depending on the approach taken. PCA based approaches generally identify a number of components in the predictor field(s) that together explain a certain amount of variance of the response data \citep[e. g. ][]{Ehrendorfer1987}. Again, this approach is very similar to the one using EOTs. In this case, the predictor field is screened for one grid-point that explains most of the variance in the entire response field. This is repeated on the reduced predictor field to identify the next important grid-point until a pre-defined number of reduced gri-point contributions is identified. Hence, for the downscaling using \pkg{Reot} it is advisable to calculate many EOTs in order to explain a large enough portion of the response variance. The exact amount obviously depends on the nature of the relationship between predictor and response fields and on the intended application. The number of EOTs needed to explain a certain, user-supplied amount of variance can easily be calculated with the \pkg{Reot} function \emph{nEot4Var()}.

In order to establish a relationship between predictor and response field, we need the two data sets (GIMMS and MODIS) to overlap for some time in order to identify pixels in the GIMMS series that, successively, are able to explain a large part of the observed variance within the MODIS series. Obviously, the longer the overlap, the better the prediction will be. In the case of GIMMS vs. MODIS, the overlap is 4 years (2003--2006). As we need to be able to evaluate the prediction performance, we split these 4 years into a training series and an evaluation series of 2 years each. Here, we simply take the first two years (2003--2004) for model training and the last two years (2005--2006) for evaluation. Of course, more sophisiticated approaches using random sampling of a number of layers are possible, but given that vegetation dynamics in the Mt. Kilimanjaro region exhibit a clear seasonal cycle, it is important that the monthly samples be equally distributed. Therefore, having two full years for the training and two full years for evaluation seems sufficient. 

Again, the data used for this example is included in the \pkg{Reot} package. It needs to be pointed out that the MODIS data set (modisKiliNDVI) was exposed to some pre-processing. Mt. Kilimanjaro is the worlds highest free standing mountain and as such has a huge influence on the local climate, including the formation of clouds. As a result, it is not unlikely to find pixels with cloud contamination as high as 90 \%. In case of NDVI observations from space, cloud contamination means missing data. In order to establish a solid relationship between the two data sets, the amount of data gaps in any of the fields needs to be minimised. For NDVI data this can be achieved by using the so-called whittaker smoother \citep{Atzberger2011} which is available as part of the \pkg{MODIS} package \citep{Mattiuzzi2013}. We applied the whittaker smoother with a lambda of 6000 and three iterations to the MODIS NDVI data. The GIMMS NDVI data is provided gap-free and hence, no pre-processing was necessary. Having eliminated missing data in both data sets, we can use \pkg{Reot} for the purpose of downscaling using GIMMS NDVI as the predictor and MODIS NDVI as the response. In order to capture a large enough amount of variability we calculate the first 10 EOTs (which together account for 95.5 \% of the variance):

<<echo = TRUE, eval = FALSE, message = FALSE, modisgimmscode1, warning=FALSE, out.width='\\linewidth', tidy = FALSE>>=
data("gimmsKiliNDVI")
data("modisKiliNDVI")

### define index for training data
pred.ind <- 1:24

### create training (pred) and evaluation (eval) sets
mod.stck.pred <- modisKiliNDVI[[pred.ind]]
mod.stck.eval <- modisKiliNDVI[[-pred.ind]]
gimms.stck.pred <- gimmsKiliNDVI[[pred.ind]]
gimms.stck.eval <- gimmsKiliNDVI[[-pred.ind]]

### calculate EOT
modes <- eot(pred = gimms.stck.pred, resp = mod.stck.pred, n = 10, 
             standardised = FALSE, reduce.both = FALSE, 
             print.console = FALSE)
@

As the result of \emph{eot()} includes all the necessary output from the regression analysis, namely the \emph{Rasters} of intercept and slope, we can create an artficial data set as a function of GIMMS NDVI with the resolution of MODIS NDVI for the evaluation time period 2005--2006. For this, we first calculate the minimum number of modes \emph{nmodes} needed to explain at least 95 \% of the response domain variance using \emph{nEot4Var()}. We then extract the respective EOT time series of each mode and create a \emph{RasterLayer} for each time slice in the GIMMS NDVI evaluation data using the calculated intercept and slope for each pixel of each mode. Thus, for each time slice we produce \emph{nmodes} layers. As these layers together represent the total variation of each time slice (or at least 95 \% of it), we need to add them together to produce the final series.

<<echo = TRUE, eval = FALSE, message = FALSE, modisgimmscode2, warning=FALSE, out.width='\\linewidth', tidy = FALSE>>=
### calculate number of modes necessary for explaining 95% variance
nmodes <- nEot4Var(modes, 0.95)

### extract identified time series (max.xy) of GIMMS evaluation set
ts.mode.eval <- sapply(seq(nmodes), function(i) {
  gimms.stck.eval[modes[[i]]$max.xy]
})

### prediction using claculated intercept, slope and GIMMS NDVI values
mod.predicted.stck <- lapply(seq(nlayers(mod.stck.eval)), function(i) {
  stack(lapply(seq(ncol(ts.mode.eval)), function(k) {
    modes[[k]]$int.response + 
      modes[[k]]$slp.response * ts.mode.eval[i, k]
  }))
})

### summate prediction for each mode at each time step
mod.predicted <- stack(lapply(seq(nrow(ts.mode.eval)), function(i) {
  calc(mod.predicted.stck[[i]], fun = sum)
}))
@

In order to assess the downscaling performance we calculate the mean error (ME), the mean absolute error (MAE), the root mean square error (RMSE), the correlation coefficient (R) and the coefficient of determination (Rsq) shown in figure \ref{fig:ndviboxplot}.

\begin{figure}[H]
\centering
\includegraphics[width=25pc]{scores_boxplots.png}
\caption{Boxplot of evaluation statistics for quality of downscaling. ME - Mean Error; MAE - Mean Absolute Error; RMSE - Root Mean Square Error; R - correlation coefficient; Rsq. - coefficient of determination.}
\label{fig:ndviboxplot}
\end{figure}

Wee see that the mean error is generally slightly negative indicating a general under-estimation of the NDVI values. However, with less than 0.1, the general prediction error is generally low as denoted by the MAE. Additionally, a median coefficient of determination of more than 0.8 indicates a rather good prediction performance.

Looking at the scatter plots for each evaluation scene (Figure \ref{fig:scatterndvi}), we see that for most scenes the NDVI values are concentrated towards the higher end of the distribution and that these are generally predicted well. Equivalently, low values are also predicted rather well. The intermediate range of NDVI values seems to be the most challenging. Looking at the temporal dynamics of the monthly scatter plots, it is indicated that there are times when these intermediate values are especially problematic (e. g. 200504, 200505, 200604 and 200611). Readers that are familiar with the general climatology of the region will immediately realise that these months represent the core times of the two rain-seasons in the Mt. Kilimanjaro area during which the NDVI behaves particularly dynamic. Especially the so-called "short rains", which usually only last for about four weeks, are vey variable and can happen anywhere between October and December. The huge underestimation of the NDVI observations during November 2006 (X200611) suggests a mismatch between the times that this short rain-season happened during the training period and the evaluation period. Given that we are investigating a heteorogeneously structured and complex arrangement of a diverse range of ecosystem types that vary tremendously over small spatial scales, it is not surprising, that the model performes somewhat worse during times of highest dynamics. This is exacerbated by the fact that there is a heteorogenic mix of natural, semi-natural and completely managed land-use systems which can exhibit completely different seasonal growing behaviour. Especially the savanna regions in the low-lands react quite rapidly to increased amounts of rain, so that they are particularly difficult to predict.

\begin{figure}[H]
\centering
\includegraphics[width=36pc]{scatter_eot.png}
\caption{Density scatter plot of observed vs. predicted NDVI values for all pixels in the 24 evaluation scenes. Black dots denote extreme outliers. The date of the scene and the coefficient of determination are shown at the bottom left in each panel. Furthermore, the regression line (solid) and the line of equality (dashed) are also shown.}
\label{fig:scatterndvi}
\end{figure}

A look at the spatial distribution of the residuals (observed - predicted) in Figure \ref{fig:residsndvi} confirms the assumption that it is especially the low-lands that are the most difficult areas for the prediction of NDVI dynamics. These regions are dominated by extensively managed agricultural crops such as maize, wheat and sunflower fields that are indeed depending on the seasonal precipitation cycle to a high degree (i. e. they are not artificially irrigated), yet exhibit different temporal responses to the local precipitation climatology than the natural savanna dominated areas. The herbacious layer of the savanna ecosystems usually responds immediately to increased water input resulting in a rather large NDVI increase over very short time periods with a gradual decrease thereafter. For the managed low-land systems this behaviour is usually reversed. 

Figure \ref{fig:residsndvi} also shows the locations of selected sites/pixels for the evaluation of the prediction performance over time (white squares). As mentioned earlier, these locations are representative of several land-cover types (with 5 replicates each). These are (in alphabetic order):

\begin{itemize}
  \item \emph{Coffee plantation (cof)}\\
  Intensively managed monoculture coffee crops including artificial irrigation as well as the use of fertilisers and pestizides, fungizides and herbizides.
  \item \emph{Ericacea forest (fer)}\\
  Small, isolated patches with remnants of Ericacea trees as high as 10 m.
  \item \emph{Lower mountain forest (flm)}\\
  The natural mountain forest ecosystem at the lower border of the forest belt.
  \item \emph{Ocotea forest (foc)}\\
  The dominant forest type at medium elevations of the forest belt.
  \item \emph{Podocarpus forest (fpo)}\\
  The dominant forest type at the higher border of the forest belt.
  \item \emph{Grasslands (gra)}\\
  Semi-natural open grasslands used for the (manual) collection of fodder to feed local livestock.
  \item \emph{Alpine Helichrysum vegetation (hel)}\\
  The natural alpine cushion vegetation at high elevations.
  \item \emph{Homegarden (hom)}\\
  The typical local multi-story agro-forestry system including beans, coffee, bananas and tall trees for shading (up to 30 m height). 
  \item \emph{Maize field (mai)}\\
  Extensively managed maize crops. No artificial irrigation, no fertilizers, pestizides, fungizides or herbizides are used.
  \item \emph{Savanna (sav)}\\
  The natural ecosystem of the low-land areas around Mt. kilimanjaro. A divers range of grassland with different densities of medium sized trees (10--20 m) such as Acacia or Combretum.
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=36pc]{resids.png}
\caption{Predicted NDVI images showing spatial distribution of residuals from the predction. Only residuals >/< 0.1 are shown. The date of the scene is shown in grey in the top right corner of each panel. The white squares show the locations of research plots for which the time series of the predicted NDVI values are shown in figure \ref{fig:tsndvi}.}
\label{fig:residsndvi}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=36pc]{dnsc_vs_mod.png}
\caption{Predicted and observed NDVI time series for selected pixels. These pixels represent different land-use types (see text for details). Black lines are predicted NDVI values, grey lines are MODIS NDVI observations. Dashed lines denote the mean of the respective series.}
\label{fig:tsndvi}
\end{figure}

As can be seen in Figure \ref{fig:tsndvi} both central tendency (dashed lines) and variance of the time series are captured well by the EOT approach. Again, the problem of accurately deriving the mean NDVI for the low-land types savanna and maize becomes obvious, though the respective variances are captured staisfactorily. In summary, the general performance of the prediction seems very reasonable, especially in light of the high complexity of the investigated region.


\section[Conclusions]{Conclusions\label{sec:conclusions}}
In this paper we have presented the \proglang{R} implementation of empirical orthogonal teleconnection analysis as introduced to the scientific community by \cite{Dool2000} in the \pkg{Reot} package. We have highlighted the general package design as well as the specific set of functions that \pkg{Reot} provides. Furthermore, we have shown a range of use-case scenarios of \pkg{Reot} in three distinct examples including the replication of one of the original examples from \cite{Dool2000}. 

Especially the utilisation of \pkg{Rcpp} for the computation intensive calculations ensures acceptable computation times and memory usage for the 'brute force' spatial data mining algorithm at hand. This is a very important aspect of \pkg{Reot} as the amount of data points in spatio-temporal geoscientific fields is generally extremely large and can easily require millions, or even billions of calculations. The general design of \pkg{Reot} ensures easy integration into potential existing work flows as it is entirely based on the \pkg{raster} package which, as the name implies, is the standard foundation for raster analyses in \proglang{R}. In its current implementation \pkg{Reot} provides all the tools to utilise the EOT algorithm as postulated by \cite{Dool2000}. For the future, we plan to enhance functionality through the integration of further link functions such as non-parametric and/or non-linear approaches. Furthermore, it is intended to extend the functionality towards providing the ability to use multiple predictor fields as well as more options for data preparation and visualisation of the resutls.

The examples provided in this paper highlight the divers range of potential uses for \pkg{Reot}. The first example is merely intended to show that the \pkg{Reot} implementation of the algorithm produces results similar to those of the original implementation by \cite{Dool2000}. Example 2 highlights the possibility to calculate EOTs across geoscientific data fields of different parameters and it is shown that it is indeed possible to capture 'real' signals. Example 3 takes a slightly different approach of using the \pkg{Reot} algorithm by using the method to statistically downscale NDVI observation form a corase to a fine gridded data set. Here, we see that some portions of the data are captured better than others, but that the general prediction performance is acceptable. In this particular example we also need to keep in mind that, especially for the forest types, the MODIS NDVI series was subjected to extensive gap-filling before the downscaling excercise. This sometimes results in unrealistic time series as is indicated for fpo2 as the most extreme example. Obviously, this will influence the quality of the regression model, so that the evaluation of the performance is somewhat distorted. Nonetheless, the general prediction performance is very encouraging.

In summray, we are confident that the introduced \pkg{Reot} package is able to expand the possibilities for researches of many disciplines to identify signals within spatio-temporal geoscientic data sets in order to assess influencial patterns of space-time variability for various applications. Furthermore, we are hopeful that \pkg{Reot} will be used in many applications other than the ones highlighted in this paper.

\section[Acknowledgements]{Acknowledgements\label{sec:acknowledgements}}
The authors would like to express their special gratitude to Huug van den Dool for providing detailed information and clarification of some of the specifics of the original code. Furthermore, the authors would like to thank the German Research Foundation (DFG) for providing some of the authors funding within the framework of the DFG-Research Unit 1246 KiLi - "Kilimanjaro ecosystems under global change: Linking biodiversity, biotic interactions and biogeochemical ecosystem processes." \url{https://www.kilimanjaro.biozentrum.uni-wuerzburg.de/Default.aspx}

\bibliography{jss_appelhans_et_al_refs}

\end{document}